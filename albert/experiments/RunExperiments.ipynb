{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALBERT Experiments for the paper \"Secure Distributed Training at Scale\"\n",
    "\n",
    "This notebook implements the ALBERT experiments for the paper. It uses a version of the [hivemind](https://github.com/learning-at-home/hivemind) library modified so that some peers may be programmed to become Byzantine and perform various types of attacks on the training process. The resulting plots are posted to the [Wandb](https://wandb.ai) service.\n",
    "\n",
    "### Step 1: Dataset Preparation\n",
    "\n",
    "1. Run the following commands locally to prepare the Wikitext103 dataset for the training:\n",
    "\n",
    "```\n",
    "git clone https://github.com/learning-at-home/hivemind\n",
    "cd hivemind\n",
    "pip install -e .\n",
    "cd examples/albert\n",
    "pip install -r requirements.txt\n",
    "python tokenize_wikitext103.py\n",
    "```\n",
    "\n",
    "2. Upload an archive with preprocessed data to some URL accessible to the AWS machines (e.g. `https://hivemind-data.s3.us-east-2.amazonaws.com/wikitext103_preprocessed.tar`).\n",
    "\n",
    "### Step 2: Setting General Parameters\n",
    "\n",
    "In this part of the notebook, you can set general parameters of your AWS configuration.\n",
    "\n",
    "It also defines `kill_instances(experiment_name)` function. You can use it to manually stop all instances related to your experiment if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "# the code below assumes that you configure boto3 with your AWS account\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html\n",
    "ec2 = boto3.resource('ec2')\n",
    "client = boto3.client('ec2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"https://\"  # TODO: Set URL of the preprocessed wikitext103 dataset from the previous step\n",
    "aws_key_name = \"\"       # TODO: Update with your aws key name\n",
    "subnet = \"\"             # TODO: Update with your subnet name or skip entirely\n",
    "security_group = \"\"     # TODO: Set security group\n",
    "WandB_API_key = \"\"      # TODO: Set wandb.ai API key\n",
    "\n",
    "image_id = \"ami-0db67995cd75f5a9f\"\n",
    "coordinator_type = \"r5.large\"\n",
    "dht_port = 31337\n",
    "num_workers = 16\n",
    "n_attackers = 7\n",
    "\n",
    "# The lower bound of the probability that a Byzantine will be revealed on the current step\n",
    "check_proba = (num_workers - n_attackers) / num_workers * 1 / (num_workers - 1)\n",
    "print(f'check_proba = {check_proba}')\n",
    "\n",
    "use_internal_routing = True  # Whether to use AWS internal (private) IP addresses\n",
    "# Note: Setting use_internal_routing = False will cause all nodes to communicate over the public network.\n",
    "#       This may incur additional charges. Change it only if you know what you're doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kill_instances(experiment_name):\n",
    "    existing_instances = ec2.instances.filter(Filters=[\n",
    "        {'Name': 'instance-state-name', 'Values': ['running']},\n",
    "        {'Name': 'tag:experiment', 'Values': [experiment_name]},\n",
    "    ])\n",
    "    ins = list(existing_instances)\n",
    "    private_ips = []\n",
    "    if ins:\n",
    "        print(f\"Already running {experiment_name}: {ins}\")\n",
    "        print(len(ins))\n",
    "        for i in ins:\n",
    "            private_ips.append(i.private_ip_address)\n",
    "            print(i.public_ip_address, i.private_ip_address)\n",
    "    \n",
    "    # to remove all instances and spot requests, run this:\n",
    "    existing_instances.terminate()\n",
    "    requests_to_shutdown = []\n",
    "    for request in client.describe_spot_instance_requests()['SpotInstanceRequests']:\n",
    "        if request['State'] == 'active' and \\\n",
    "                any(tag['Key'] == 'experiment' and tag['Value'] == experiment_name for tag in request['Tags']):\n",
    "            requests_to_shutdown.append(request['SpotInstanceRequestId'])\n",
    "    if requests_to_shutdown:\n",
    "        client.cancel_spot_instance_requests(\n",
    "            SpotInstanceRequestIds=requests_to_shutdown)\n",
    "    print('Instances stopped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define Coordinator Setup Code\n",
    "\n",
    "A coordinator is an instance that welcomes new peers into a decentralized training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ip_cmd = \"export IP=$(ifconfig eth0 | grep -Eo 'inet (addr:)?([0-9]*\\.){3}[0-9]*' | grep -Eo '([0-9]*\\.){3}[0-9]*')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinator_script = f'''#!/bin/bash -ex\n",
    "exec > >(tee /var/log/user-command.log|logger -t user-data -s 2>/dev/console) 2>&1\n",
    "\n",
    "# note: we configure rsyslog to forward logs from all trainers\n",
    "sudo sh -c 'cat <<\"EOF\" >> /etc/rsyslog.conf\n",
    "$ModLoad imudp\n",
    "$UDPServerRun 514\n",
    "\n",
    "$ModLoad imtcp\n",
    "$InputTCPServerRun 514\n",
    "\n",
    "$FileCreateMode 0644\n",
    "$DirCreateMode 0755\n",
    "$Umask 0022\n",
    "\n",
    "$template RemoteLogs,\"/var/log/rsyslog/%HOSTNAME%.log\"\n",
    "*.*  ?RemoteLogs\n",
    "& ~\n",
    "EOF'\n",
    "sudo systemctl restart rsyslog\n",
    "\n",
    "{get_ip_cmd if use_internal_routing else ''}\n",
    "\n",
    "# NOTE: docker run must be called without --it as there is no tty\n",
    "# check machine's /var/log/user-command.log for details\n",
    "\n",
    "docker run --name trainer_run --ipc=host --net=host learningathome/hivemind:master bash -c \"\"\"\n",
    "set -euxo pipefail\n",
    "\n",
    "rm -rf hivemind\n",
    "git clone https://github.com/yandex-research/btard\n",
    "cd btard/albert/hivemind\n",
    "\n",
    "pip install -e .\n",
    "pip install transformers==4.5.1\n",
    "cd ../experiments\n",
    "\n",
    "\n",
    "ulimit -n 4096\n",
    "\n",
    "\n",
    "sh -c 'cat <<\"EOF\" >> ~/.netrc\n",
    "machine api.wandb.ai\n",
    "  login user\n",
    "  password {WandB_API_key}\n",
    "EOF'\n",
    "\n",
    "\n",
    "if [ \"%initial_state_url%\" != \"\" ]; then\n",
    "    wget -q \"%initial_state_url%\" -O initial_state.pickle\n",
    "fi\n",
    "\n",
    "HIVEMIND_TAU=%tau% HIVEMIND_THREADS=256 WANDB_ENTITY=learning-at-home python ./run_first_peer.py --dht_listen_on [::]:{dht_port} {'--address $IP' if use_internal_routing else ''} \\\n",
    " --experiment_prefix %experiment_name% --wandb_project Runs \\\n",
    " --compression NONE --metadata_expiration 180 --averaging_timeout 60 --averaging_expiration 10 \\\n",
    " $(if [ \"%initial_step%\" != \"0\" ]; then echo --initial_state_path initial_state.pickle; fi)\n",
    "\"\"\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_coordinator(tau, experiment_name, initial_step=0, initial_state_url=''):\n",
    "    coordinator, = ec2.create_instances(\n",
    "        ImageId=image_id, InstanceType=coordinator_type,\n",
    "        MinCount=1, MaxCount=1,\n",
    "        SecurityGroupIds=[security_group], SubnetId=subnet,\n",
    "        KeyName=aws_key_name,\n",
    "        UserData=coordinator_script\n",
    "            .replace('%tau%', str(tau))\n",
    "            .replace('%initial_state_url%', initial_state_url)\n",
    "            .replace('%initial_step%', str(initial_step))\n",
    "            .replace('%experiment_name%', experiment_name),\n",
    "        TagSpecifications=[{'ResourceType': 'instance', 'Tags': [\n",
    "            {'Key':'experiment', 'Value': experiment_name},\n",
    "            {'Key':'role', 'Value': 'first_peer'}\n",
    "        ]}]\n",
    "    )\n",
    "    coordinator.wait_until_running()\n",
    "    coordinator, = list(ec2.instances.filter(InstanceIds=[coordinator.id]))\n",
    "\n",
    "    print('Created coordinator:', coordinator.private_ip_address, coordinator.public_ip_address)\n",
    "\n",
    "    if use_internal_routing:\n",
    "        coordinator_ip = coordinator.private_ip_address\n",
    "    else:\n",
    "        coordinator_ip = coordinator.public_ip_address\n",
    "\n",
    "    coordinator_endpoint = f\"{coordinator_ip}:{dht_port}\"\n",
    "    print('coordinator_endpoint =', coordinator_endpoint)\n",
    "    \n",
    "    return {'ip': coordinator_ip, 'endpoint': coordinator_endpoint}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define Worker Setup Code\n",
    "\n",
    "Workers are preemptible GPU instances that run compute gradients and perform averaging. In this example, each worker is a single tesla T4 instance.\n",
    "\n",
    "You will typically run 16 workers. Some of them may be Byzantine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_script = f'''#!/bin/bash -ex\n",
    "exec > >(tee /var/log/user-command.log|logger -t user-data -s 2>/dev/console) 2>&1\n",
    "\n",
    "set -euxo pipefail\n",
    "cd ~\n",
    "\n",
    "sudo sh -c 'cat <<\"EOF\" >> /etc/rsyslog.conf\n",
    "\n",
    "user.* @@%coordinator_ip%:514\n",
    "\n",
    "EOF'\n",
    "sudo systemctl restart rsyslog\n",
    "\n",
    "\n",
    "{get_ip_cmd if use_internal_routing else ''}\n",
    "\n",
    "\n",
    "docker run --name hivemind_run --gpus all --ipc=host --net=host learningathome/hivemind:master bash -c \"\"\"\n",
    "\n",
    "rm -rf hivemind\n",
    "git clone https://github.com/yandex-research/btard\n",
    "cd btard/albert/hivemind\n",
    "\n",
    "pip install -e .\n",
    "pip install transformers==4.5.1\n",
    "cd ../experiments\n",
    "\n",
    "\n",
    "mkdir -p ~/data\n",
    "wget -qO- {data_path} | tar xzf -\n",
    "\n",
    "\n",
    "sh -c 'cat <<\"EOF\" >> ~/.netrc\n",
    "machine api.wandb.ai\n",
    "  login user\n",
    "  password {WandB_API_key}\n",
    "EOF'\n",
    "\n",
    "\n",
    "ulimit -n 4096\n",
    "\n",
    "HIVEMIND_TAU=%tau% ATTACK_TYPE=%attack_type% ATTACK_START=%attack_start% CHECK_PROBA={check_proba} \\\n",
    "  DIRECTION_SEED=%seed% \\\n",
    "  WANDB_PROJECT=%experiment_name% WANDB_ENTITY=learning-at-home WANDB_WATCH=false \\\n",
    "  HIVEMIND_THREADS=256 python run_trainer.py \\\n",
    "  --output_dir ./outputs --overwrite_output_dir \\\n",
    "  {'--endpoint $IP'+':*' if use_internal_routing else ''} \\\n",
    "  --logging_dir ./logs --logging_first_step --logging_steps 100 \\\n",
    "  --initial_peers %coordinator_endpoint%  --run_name aws_worker \\\n",
    "  --experiment_prefix %experiment_name% --seed %seed% --compression NONE --metadata_expiration 180 \\\n",
    "  --averaging_timeout 60 --averaging_expiration 10 --statistics_expiration 60\n",
    "\"\"\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instance(worker_type, attack_type, tau, experiment_name, coordinator_ip, coordinator_endpoint, seed,\n",
    "                    attack_start):\n",
    "    new_worker, = ec2.create_instances(\n",
    "    ImageId=image_id, InstanceType=worker_type,\n",
    "    MinCount=1, MaxCount=1,\n",
    "    UserData=worker_script\n",
    "        .replace('%attack_type%', attack_type)\n",
    "        .replace('%attack_start%', str(attack_start))\n",
    "        .replace('%tau%', str(tau))\n",
    "        .replace('%tau_underscore%', str(tau).replace('.', '_'))\n",
    "        .replace('%experiment_name%', experiment_name)\n",
    "        .replace('%coordinator_ip%', coordinator_ip)\n",
    "        .replace('%coordinator_endpoint%', coordinator_endpoint)\n",
    "        .replace('%seed%', str(seed)),\n",
    "    SecurityGroupIds=[security_group], SubnetId=subnet, \n",
    "    KeyName=aws_key_name,\n",
    "    InstanceMarketOptions={\n",
    "        \"MarketType\": \"spot\",\n",
    "        \"SpotOptions\": {\n",
    "            \"SpotInstanceType\": \"one-time\",\n",
    "            \"InstanceInterruptionBehavior\": \"terminate\"\n",
    "        }\n",
    "    },\n",
    "    TagSpecifications=[{'ResourceType': 'instance', 'Tags': [\n",
    "        {'Key':'experiment', 'Value': experiment_name},\n",
    "        {'Key':'role', 'Value': 'gpu_worker'}\n",
    "    ]}, {'ResourceType': 'spot-instances-request', 'Tags': [\n",
    "        {'Key':'experiment', 'Value': experiment_name},\n",
    "        {'Key':'role', 'Value': 'gpu_worker'}\n",
    "    ]}],)\n",
    "    return new_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import traceback\n",
    "\n",
    "def run_workers(tau, experiment_name, coordinator_ip, coordinator_endpoint,\n",
    "                n_attackers, time_limit=3 * 3600, intended_attack='NONE', **kwargs):\n",
    "    stop_time = time.time() + time_limit\n",
    "    while time.time() < stop_time:\n",
    "        existing_instances = list(ec2.instances.filter(Filters=[\n",
    "            {'Name': 'instance-state-name', 'Values': ['running']},\n",
    "            {'Name': 'tag:experiment', 'Values': [experiment_name]},\n",
    "        ]))\n",
    "\n",
    "        count_needed = num_workers + 1 - len(existing_instances)\n",
    "        if count_needed > 0:\n",
    "            attack_type = intended_attack if count_needed > num_workers - n_attackers else 'NONE'\n",
    "            \n",
    "            print(f\"Need {count_needed} more workers. Trying to spawn one\")\n",
    "            instance_types = ['g4dn.2xlarge']\n",
    "            for i, worker_type in enumerate(instance_types):\n",
    "                try:\n",
    "                    new_worker = create_instance(\n",
    "                        worker_type, attack_type, tau, experiment_name, coordinator_ip, coordinator_endpoint, **kwargs)\n",
    "                    new_worker.wait_until_running()\n",
    "                    new_worker, = list(ec2.instances.filter(InstanceIds=[new_worker.id]))\n",
    "                    print(\"CREATED ONE WORKER!\", worker_type, attack_type,\n",
    "                          new_worker.public_ip_address, new_worker.private_ip_address)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print('Failed:', worker_type, e)\n",
    "                    traceback.print_exc()\n",
    "                    \n",
    "        time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Running the Training from Scratch\n",
    "\n",
    "This steps runs the training from scratch, so you will be able to collect checkpoints for the steps 950 and 4950, where we are going to test different attack types. The training from scratch is expected to run for $\\approx 3$ days. It should be done for each value of $\\tau$ (the CenteredClip parameter) separately.\n",
    "\n",
    "Once the workers have started (7-10 minutes into training), you will be able to see the training progress in your Wandb account:\n",
    "\n",
    "<img src=\"images/scratch_wandb.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 0.125  # TODO: Set tau for CenteredClip\n",
    "seed = 1337\n",
    "attack_start = 0\n",
    "intended_attack = 'NONE'\n",
    "\n",
    "experiment_name = \"baseline_tau_{tau}\"\n",
    "\n",
    "try:\n",
    "    print(f'\\n[*] {experiment_name}: Creating coordinator...')\n",
    "    while True:\n",
    "        try:\n",
    "            coordinator_info = create_coordinator(tau, experiment_name)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print('[-] Failed to create coordinator:', e)\n",
    "            traceback.print_exc()\n",
    "            time.sleep(30)\n",
    "    time.sleep(5 * 60)\n",
    "\n",
    "    print(f'\\n[*] {experiment_name}: Running workers...')\n",
    "    run_workers(tau, experiment_name, coordinator_info['ip'], coordinator_info['endpoint'], n_attackers,\n",
    "                time_limit=7 * 24 * 3600, intended_attack=intended_attack, seed=seed, attack_start=attack_start)\n",
    "finally:\n",
    "    print(f'\\n[*] {experiment_name}: Stopping instances...')\n",
    "    kill_instances(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should wait until the training reaches $\\approx 5000$ steps:\n",
    "\n",
    "<img src=\"images/convergence_wandb.png\" width=\"500\">\n",
    "\n",
    "Then you can obtain the model checkpoints dumped for the steps 950 and 4950:\n",
    "\n",
    "1. Connect to any worker using SSH and its public IP from the previous cell's output;\n",
    "2. Attach to the Docker container: `docker exec -it hivemind_run bash`;\n",
    "3. Download the checkpoints (`*.pickle` files) from the hivemind directory inside Docker to your local machine;\n",
    "4. Upload them to some URL accessible to the AWS machines.\n",
    "\n",
    "After that, you can wait until the model converges or move on to testing various attack types.\n",
    "\n",
    "### Step 6: Simulating Various Attack Types\n",
    "\n",
    "Now, you can load one of the checkpoints and simulate different attack types (sign flipping, label flipping, random direction, or the absence of any attack) at this stage of the training. One attack simulation runs for 4 hours and makes $\\approx 250$ training steps. It makes sense to run one simulation multiple times with different $seed$ values to learn the possible loss variance and observe worst case scenarios for the current attack.\n",
    "\n",
    "Similarly to the previous step, each simulation generates a separate Wandb experiment. The loss curve will demonstrate that this time the experiment begins midway through training and the attack temporarily increases the loss value after the step number `attack_start`.\n",
    "\n",
    "On the other figure, you will see that all Byzantine peers will be revealed and banned over the time. In real BTARD-SGD, this would happen when a randomly chosen validating peer will discover that a Byzantine has sent fake gradients. However, in our simulation, a Byzantine just bans itself with the probability `check_proba` (that is chosen to be the lower bound of the probability that it will be revealed in real BTARD-SGD with one validator).\n",
    "\n",
    "<img src=\"images/attack_wandb.png\" width=\"750\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state_url = \"https://\"     # TODO: Set the URL to the checkpoint obtained on the previous step\n",
    "seed = 1337                        # TODO: Set random seed\n",
    "tau = 0.125                        # TODO: Set tau for CenteredClip\n",
    "initial_step = 950                 # TODO: Set initial step (which state is to be loaded)\n",
    "attack_start = 1000                # TODO: Set the step where the attack starts\n",
    "intended_attack = 'SIGN_FLIPPING'  # TODO: Set the attack_type\n",
    "                                   #       (one of NONE, SIGN_FLIPPING, LABEL_FLIPPING, or CONSTANT_DIRECTION)\n",
    "\n",
    "experiment_name = f\"{intended_attack.lower()}_at_{initial_step}_tau_{tau}_seed_{seed}\"\n",
    "\n",
    "try:\n",
    "    print(f'\\n[*] {experiment_name}: Creating coordinator...')\n",
    "    while True:\n",
    "        try:\n",
    "            coordinator_info = create_coordinator(tau, experiment_name,\n",
    "                                                  initial_step=initial_step, initial_state_url=initial_state_url)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print('[-] Failed to create coordinator:', e)\n",
    "            traceback.print_exc()\n",
    "            time.sleep(30)\n",
    "    time.sleep(5 * 60)\n",
    "\n",
    "    print(f'\\n[*] {experiment_name}: Running workers...')\n",
    "    run_workers(tau, experiment_name, coordinator_info['ip'], coordinator_info['endpoint'], n_attackers,\n",
    "                time_limit=4 * 3600, intended_attack=intended_attack, seed=seed, attack_start=attack_start)\n",
    "finally:\n",
    "    print(f'\\n[*] {experiment_name}: Stopping instances...')\n",
    "    kill_instances(experiment_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
